{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from dotenv import load_dotenv\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "if not load_dotenv():\n",
    "    raise Exception(\"No .env file found\")\n",
    "else:\n",
    "    print(\"Environment variables loaded.\")\n",
    "from evaluation import generate_evaluation_dataset, display_confusion_matrix, evaluate_classification_model_on_dataset\n",
    "from inference import batch_inference, generate_test_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configurer le modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_models = [\"gpt-4o\", \"gpt-4o-mini\"]\n",
    "while True:\n",
    "    llm_model = input(\"Quel modèle voulez-vous utiliser ? Choix : gpt-4o-mini, gpt-4o\\n\").strip().lower()\n",
    "    if llm_model in valid_models:\n",
    "        break\n",
    "    print(\"Modèle invalide. Veuillez entrer 'gpt-4o' ou 'gpt-4o-mini'.\")\n",
    "concept = input(\"Quel critère de classification voulez-vous utiliser ? sujet, sentiment, etc.\")\n",
    "class_1 = input(\"Quelle est la première classe à considérer ?\")\n",
    "class_2 = input(\"Quelle est la deuxième classe à considérer ?\")\n",
    "\n",
    "model_parameters = {\n",
    "    \"llm_model\": llm_model,\n",
    "    \"concept\": concept,\n",
    "    \"class_1\": class_1,\n",
    "    \"class_2\": class_2\n",
    "}\n",
    "print(f\"Le modèle utilise les paramètres suivants :\\n- LLM : {llm_model}\\n- Critère de classification : {concept}\\n- Classes : {class_1} | {class_2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Évaluer le modèle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour évaluer le modèle, vous pouvez fournir un fichier au format texte (.txt) contenant les verbatim à classifier et leurs classes associées.\n",
    "\n",
    "Chaque ligne du fichier doit contenir un verbatim suivi de sa classe associée, séparés par \" // \".<br>Par exemple :\n",
    "\n",
    "Quel est le prix de la chaise ? // prix<br>\n",
    "Je veux payer ma facture. // paiement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_answer = [\"oui\", \"non\"]\n",
    "while True:\n",
    "    validation_file = input(\"Avez-vous un fichier de verbatim (.txt) pour évaluer le modèle ? (oui/non)\").strip().lower()\n",
    "    if validation_file in valid_answer:\n",
    "        break\n",
    "    print(\"Réponse invalide. Veuillez entrer 'oui' ou 'non'.\")\n",
    "if validation_file == \"oui\":\n",
    "    validation_file = input(\"Veuillez fournir le chemin du fichier : \")\n",
    "    with open(validation_file, \"r\") as f:\n",
    "        validation_file = f.readlines()\n",
    "else:\n",
    "    while True:\n",
    "        try:\n",
    "            num_samples = int(input(\"Pas de souci, nous allons générer un dataset d'évaluation. Combien de verbatim souhaitez-vous générer ? \"))\n",
    "            break\n",
    "        except ValueError:\n",
    "            print(\"Veuillez entrer un nombre entier valide.\\n\")\n",
    "    generated_dataset = generate_evaluation_dataset(llm_model=llm_model, concept=concept, class_1=class_1, class_2=class_2, num_samples=num_samples)\n",
    "    with open(f\"datasets/evaluation_dataset_{concept}_{time.strftime(\"%d_%m_%Y_%H_%M\")}.txt\", \"w\") as f:\n",
    "        f.write(generated_dataset)\n",
    "    validation_file = generated_dataset.split(\"\\n\")\n",
    "    \n",
    "print(\"Aperçu :\\n\")\n",
    "for line in validation_file[:5]:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions, ground_truths = evaluate_classification_model_on_dataset(model_parameters, validation_file)\n",
    "\n",
    "accuracy = accuracy_score(ground_truths, predictions)\n",
    "print(f\"\\nAccuracy: {accuracy:.2f}\\n\")\n",
    "print(f\"Confusion matrix for classification criterion : {concept}\")\n",
    "display_confusion_matrix(ground_truths, predictions, labels=[class_1, class_2])\n",
    "report = classification_report(ground_truths, predictions, labels=[class_1, class_2])\n",
    "print(f\"F1, recall and precision scores:\\n\\n{report}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utiliser le modèle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour utiliser les prédictions du modèle, vous pouvez fournir un fichier au format texte (.txt) contenant les verbatim à classifier. <br>\n",
    "Le modèle va prédire la classe de chaque verbatim et expliquer la raison de la prédiction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_answer = [\"oui\", \"non\"]\n",
    "while True:\n",
    "    test_file = input(\"Avez-vous un fichier de verbatim (.txt) pour tester le modèle ? (oui/non)\").strip().lower()\n",
    "    if test_file in valid_answer:\n",
    "        break\n",
    "    print(\"Réponse invalide. Veuillez entrer 'oui' ou 'non'.\")\n",
    "\n",
    "if test_file.lower() == \"oui\":\n",
    "    test_file = input(\"Veuillez fournir le chemin du fichier : \")\n",
    "    with open(test_file, \"r\") as f:\n",
    "        test_file = f.readlines()\n",
    "else:\n",
    "    while True:\n",
    "        try:\n",
    "            num_samples = int(input(\"Pas de souci, nous allons générer un dataset de test. Combien de verbatim souhaitez-vous générer ? \"))\n",
    "            break\n",
    "        except ValueError:\n",
    "            print(\"Veuillez entrer un nombre entier valide.\\n\")\n",
    "    generated_dataset = generate_test_dataset(llm_model=llm_model, concept=concept, class_1=class_1, class_2=class_2, num_samples=num_samples)\n",
    "    with open(f\"datasets/test_dataset_{concept}_{time.strftime(\"%d_%m_%Y_%H_%M\")}.txt\", \"w\") as f:\n",
    "        f.write(generated_dataset)\n",
    "    test_file = generated_dataset.split(\"\\n\")\n",
    "    \n",
    "print(\"Aperçu :\\n\")\n",
    "for line in test_file[:5]:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = batch_inference(model_parameters=model_parameters, inputs=test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for prediction in predictions:\n",
    "    print(f\"Verbatim : {prediction[0]}\")\n",
    "    print(f\"- Prédiction : {prediction[1]}\")\n",
    "    print(f\"- Raison : {prediction[2]}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
